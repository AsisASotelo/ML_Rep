{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are the fundamental components of Random Forests, which are the most powerful Machine Learning Algorithms available today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,2:]\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of a decision tree can be done with sklearn especifically the export_graphviz() module from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf,\n",
    "               out_file=\"iris_tree.dot\",\n",
    "               feature_names = iris.feature_names[2:],\n",
    "               class_names=iris.target_names,\n",
    "               rounded=True,\n",
    "               filled=True\n",
    "            )\n",
    "# Note that you can convert the output dot file to png and \n",
    "# jpg format using the command \n",
    "# $ dot -Tpng iris_tree.dot -o iris_tree.png\n",
    "\n",
    "# Note that you must download the graphviz package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5,1.5]])\n",
    "tree_clf.predict([[5,1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Classification and Regression Tree, also called growing trees. The algorithm splits the training set into two subsets using a single feature k and a threshold tk that produces the purest subsets (weighed by their size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(k,t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here m is the number of instances in the left and right divided by the total number of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each node only requires checking the value of one feature, the overall prediction complexity is just $$O(log_2(m))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entryopy Vs. Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is an impurity measure: a set's entropy is zero when it contains instances of only one class. Usually they lead to similar trees but Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce more balanced trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H_i = - \\sum_{k=1}^n p_{i,k}log(p_{i,k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tree is nonparametric model, not because it does not have parameters but because the number of parameters is not determined prior to training. This means it is more prone to overfitting.\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decisions Tree freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
